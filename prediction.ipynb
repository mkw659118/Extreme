{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d71b0949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "def insert_pred_to_sql():\n",
    "    # 读取数据库连接字符串\n",
    "    with open('./datasets/sql_token.pkl', 'rb') as f:\n",
    "        DB_URI = pickle.load(f)\n",
    "\n",
    "    # 创建数据库引擎\n",
    "    engine = create_engine(DB_URI)\n",
    "\n",
    "    # SQL 查询语句\n",
    "    sql = text(\"\"\"\n",
    "        SELECT fund_code, forecast_date, pre_data, model_version, create_time, update_time\n",
    "        FROM b_fund_forecast_new\n",
    "        WHERE fund_code IN :codes\n",
    "        ORDER BY forecast_date\n",
    "    \"\"\")\n",
    "\n",
    "    # 执行查询，传入参数（注意 tuple 中只有一个元素时加逗号）\n",
    "    df = pd.read_sql_query(\n",
    "        sql.bindparams(codes=tuple(['005626'])),  # 或 codes=('005626',)\n",
    "        engine\n",
    "    )\n",
    "    \n",
    "    df.to_sql('my_table', engine, if_exists='replace', index=False)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9126f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np \n",
    "# for i in range(len(df)):\n",
    "#     pred_str = df['pre_data'][i]\n",
    "#     str_start = pred_str.find('[') + 1\n",
    "#     str_end = pred_str.find(']') \n",
    "#     pred = pred_str[str_start:str_end].split(', ')\n",
    "#     pred = np.array(pred, dtype=np.float32)\n",
    "#     # print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f9f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_start_date(end_date: str, window_size: int) -> str:\n",
    "    \"\"\"\n",
    "    给定结束日期和历史窗口长度，返回窗口开始日期（字符串格式）。\n",
    "\n",
    "    参数：\n",
    "    - end_date (str): 结束日期，格式 'YYYY-MM-DD'\n",
    "    - window_size (int): 历史窗口长度（天数）\n",
    "\n",
    "    返回：\n",
    "    - start_date (str): 开始日期，格式 'YYYY-MM-DD'\n",
    "    \"\"\"\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    start_dt = end_dt - timedelta(days=window_size)\n",
    "    return start_dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5123636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_provider.generate_financial import process_date_columns, query_fund_data\n",
    "from data_provider.get_financial import get_group_idx\n",
    "from utils.exp_config import get_config\n",
    "import pickle \n",
    "from sqlalchemy import create_engine, text\n",
    "import numpy as np\n",
    "\n",
    "config = get_config('FinancialConfig')\n",
    "\n",
    "# 读取数据库连接字符串\n",
    "with open('./datasets/sql_token.pkl', 'rb') as f:\n",
    "    DB_URI = pickle.load(f)\n",
    "\n",
    "# 创建数据库引擎\n",
    "engine = create_engine(DB_URI)\n",
    "now_fund_code = get_group_idx(27)\n",
    "end_date = '2025-4-15'\n",
    "all_history_input = []\n",
    "for i in range(len(now_fund_code)):\n",
    "    start_date = get_start_date(end_date, window_size=64)\n",
    "    df = query_fund_data(now_fund_code[i], start_date, end_date)\n",
    "    df = process_date_columns(df)\n",
    "    all_history_input.append(df)\n",
    "\n",
    "df = all_history_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e237ab5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 68, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.stack(df, axis=0)\n",
    "data = data.transpose(1, 0, 2)\n",
    "x = data[:, :, :]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48d32fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from datetime import datetime\n",
    "\n",
    "# def convert_array_to_df(array):\n",
    "#     \"\"\"\n",
    "#     将数组转换为包含标准日期列的 DataFrame\n",
    "#     \"\"\"\n",
    "#     df = pd.DataFrame(array, columns=[\n",
    "#         'fund_code', 'year', 'month', 'day', 'weekday',\n",
    "#         'value1', 'value2', 'value3'\n",
    "#     ])\n",
    "#     df['year'] = df['year'].astype(int)\n",
    "#     df['month'] = df['month'].astype(int)\n",
    "#     df['day'] = df['day'].astype(int)\n",
    "#     df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "#     return df\n",
    "\n",
    "# def find_missing_dates_from_array_list(array_list, start_date, end_date, freq='B'):\n",
    "#     \"\"\"\n",
    "#     参数：\n",
    "#     - array_list: List[np.ndarray]，每个元素是一个基金的历史数据数组\n",
    "#     - start_date, end_date: 'YYYY-MM-DD'\n",
    "#     - freq: 'B' 表示工作日（默认）\n",
    "\n",
    "#     返回：\n",
    "#     - dict: {fund_code: [缺失的日期列表]}\n",
    "#     \"\"\"\n",
    "#     full_range = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "#     missing_map = {}\n",
    "\n",
    "#     for arr in array_list:\n",
    "#         df = convert_array_to_df(arr)\n",
    "#         if df.empty:\n",
    "#             continue\n",
    "\n",
    "#         fund_code = df.iloc[0]['fund_code']\n",
    "#         date_series = df['date']\n",
    "#         missing = full_range.difference(date_series)\n",
    "\n",
    "#         if not missing.empty:\n",
    "#             missing_map[fund_code] = missing.strftime('%Y-%m-%d').tolist()\n",
    "\n",
    "#     return missing_map\n",
    "\n",
    "# missing_map = find_missing_dates_from_array_list(df, start_date, end_date)\n",
    "\n",
    "# for code, dates in missing_map.items():\n",
    "#     print(f\"基金 {code} 缺失日期：{dates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a799e87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 68, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.stack(df, axis=0)\n",
    "data = data.transpose(1, 0, 2)\n",
    "x = data[:, :, :]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd564219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2255954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in state_dict:\n",
      "model.projection.weight : torch.Size([56, 3])\n",
      "model.projection.bias : torch.Size([56])\n",
      "model.position_embedding.pos_encoding.embedding.weight : torch.Size([17, 56])\n",
      "model.fund_embedding.weight : torch.Size([999999, 56])\n",
      "model.predict_linear.weight : torch.Size([26, 16])\n",
      "model.predict_linear.bias : torch.Size([26])\n",
      "model.encoder.layers.0.0.weight : torch.Size([56])\n",
      "model.encoder.layers.0.1.att.in_proj_weight : torch.Size([168, 56])\n",
      "model.encoder.layers.0.1.att.in_proj_bias : torch.Size([168])\n",
      "model.encoder.layers.0.1.att.out_proj.weight : torch.Size([56, 56])\n",
      "model.encoder.layers.0.1.att.out_proj.bias : torch.Size([56])\n",
      "model.encoder.layers.0.2.weight : torch.Size([56])\n",
      "model.encoder.layers.0.3.net.0.weight : torch.Size([112, 56])\n",
      "model.encoder.layers.0.3.net.0.bias : torch.Size([112])\n",
      "model.encoder.layers.0.3.net.2.weight : torch.Size([56, 112])\n",
      "model.encoder.layers.0.3.net.2.bias : torch.Size([56])\n",
      "model.encoder.layers.1.0.weight : torch.Size([56])\n",
      "model.encoder.layers.1.1.att.in_proj_weight : torch.Size([168, 56])\n",
      "model.encoder.layers.1.1.att.in_proj_bias : torch.Size([168])\n",
      "model.encoder.layers.1.1.att.out_proj.weight : torch.Size([56, 56])\n",
      "model.encoder.layers.1.1.att.out_proj.bias : torch.Size([56])\n",
      "model.encoder.layers.1.2.weight : torch.Size([56])\n",
      "model.encoder.layers.1.3.net.0.weight : torch.Size([112, 56])\n",
      "model.encoder.layers.1.3.net.0.bias : torch.Size([112])\n",
      "model.encoder.layers.1.3.net.2.weight : torch.Size([56, 112])\n",
      "model.encoder.layers.1.3.net.2.bias : torch.Size([56])\n",
      "model.encoder.norm.weight : torch.Size([56])\n",
      "model.encoder2.layers.0.0.weight : torch.Size([56])\n",
      "model.encoder2.layers.0.1.att.in_proj_weight : torch.Size([168, 56])\n",
      "model.encoder2.layers.0.1.att.in_proj_bias : torch.Size([168])\n",
      "model.encoder2.layers.0.1.att.out_proj.weight : torch.Size([56, 56])\n",
      "model.encoder2.layers.0.1.att.out_proj.bias : torch.Size([56])\n",
      "model.encoder2.layers.0.2.weight : torch.Size([56])\n",
      "model.encoder2.layers.0.3.net.0.weight : torch.Size([112, 56])\n",
      "model.encoder2.layers.0.3.net.0.bias : torch.Size([112])\n",
      "model.encoder2.layers.0.3.net.2.weight : torch.Size([56, 112])\n",
      "model.encoder2.layers.0.3.net.2.bias : torch.Size([56])\n",
      "model.encoder2.layers.1.0.weight : torch.Size([56])\n",
      "model.encoder2.layers.1.1.att.in_proj_weight : torch.Size([168, 56])\n",
      "model.encoder2.layers.1.1.att.in_proj_bias : torch.Size([168])\n",
      "model.encoder2.layers.1.1.att.out_proj.weight : torch.Size([56, 56])\n",
      "model.encoder2.layers.1.1.att.out_proj.bias : torch.Size([56])\n",
      "model.encoder2.layers.1.2.weight : torch.Size([56])\n",
      "model.encoder2.layers.1.3.net.0.weight : torch.Size([112, 56])\n",
      "model.encoder2.layers.1.3.net.0.bias : torch.Size([112])\n",
      "model.encoder2.layers.1.3.net.2.weight : torch.Size([56, 112])\n",
      "model.encoder2.layers.1.3.net.2.bias : torch.Size([56])\n",
      "model.encoder2.norm.weight : torch.Size([56])\n",
      "model.decoder.weight : torch.Size([1, 56])\n",
      "model.decoder.bias : torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_411756/2551740208.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load('checkpoints/ours/Model_ours_Dataset_financial_Multi_round_0.pt')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "state = torch.load('checkpoints/ours/Model_ours_Dataset_financial_Multi_round_0.pt')\n",
    "\n",
    "if isinstance(state, dict):\n",
    "    # 一般是 state_dict\n",
    "    print(\"Keys in state_dict:\")\n",
    "    for key, value in state.items():\n",
    "        print(f\"{key} : {value.shape}\")\n",
    "else:\n",
    "    # 如果是完整模型\n",
    "    print(state)  # 会自动打印结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac517d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_411756/804212040.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = Backbone(3, config).load_state_dict(torch.load('checkpoints/ours/Model_ours_Dataset_financial_Multi_round_0.pt'))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Backbone:\n\tMissing key(s) in state_dict: \"projection.weight\", \"projection.bias\", \"position_embedding.pos_encoding.embedding.weight\", \"fund_embedding.weight\", \"predict_linear.weight\", \"predict_linear.bias\", \"encoder.layers.0.0.weight\", \"encoder.layers.0.1.att.in_proj_weight\", \"encoder.layers.0.1.att.in_proj_bias\", \"encoder.layers.0.1.att.out_proj.weight\", \"encoder.layers.0.1.att.out_proj.bias\", \"encoder.layers.0.2.weight\", \"encoder.layers.0.3.net.0.weight\", \"encoder.layers.0.3.net.0.bias\", \"encoder.layers.0.3.net.2.weight\", \"encoder.layers.0.3.net.2.bias\", \"encoder.layers.1.0.weight\", \"encoder.layers.1.1.att.in_proj_weight\", \"encoder.layers.1.1.att.in_proj_bias\", \"encoder.layers.1.1.att.out_proj.weight\", \"encoder.layers.1.1.att.out_proj.bias\", \"encoder.layers.1.2.weight\", \"encoder.layers.1.3.net.0.weight\", \"encoder.layers.1.3.net.0.bias\", \"encoder.layers.1.3.net.2.weight\", \"encoder.layers.1.3.net.2.bias\", \"encoder.norm.weight\", \"encoder2.layers.0.0.weight\", \"encoder2.layers.0.1.att.in_proj_weight\", \"encoder2.layers.0.1.att.in_proj_bias\", \"encoder2.layers.0.1.att.out_proj.weight\", \"encoder2.layers.0.1.att.out_proj.bias\", \"encoder2.layers.0.2.weight\", \"encoder2.layers.0.3.net.0.weight\", \"encoder2.layers.0.3.net.0.bias\", \"encoder2.layers.0.3.net.2.weight\", \"encoder2.layers.0.3.net.2.bias\", \"encoder2.layers.1.0.weight\", \"encoder2.layers.1.1.att.in_proj_weight\", \"encoder2.layers.1.1.att.in_proj_bias\", \"encoder2.layers.1.1.att.out_proj.weight\", \"encoder2.layers.1.1.att.out_proj.bias\", \"encoder2.layers.1.2.weight\", \"encoder2.layers.1.3.net.0.weight\", \"encoder2.layers.1.3.net.0.bias\", \"encoder2.layers.1.3.net.2.weight\", \"encoder2.layers.1.3.net.2.bias\", \"encoder2.norm.weight\", \"decoder.weight\", \"decoder.bias\". \n\tUnexpected key(s) in state_dict: \"model.projection.weight\", \"model.projection.bias\", \"model.position_embedding.pos_encoding.embedding.weight\", \"model.fund_embedding.weight\", \"model.predict_linear.weight\", \"model.predict_linear.bias\", \"model.encoder.layers.0.0.weight\", \"model.encoder.layers.0.1.att.in_proj_weight\", \"model.encoder.layers.0.1.att.in_proj_bias\", \"model.encoder.layers.0.1.att.out_proj.weight\", \"model.encoder.layers.0.1.att.out_proj.bias\", \"model.encoder.layers.0.2.weight\", \"model.encoder.layers.0.3.net.0.weight\", \"model.encoder.layers.0.3.net.0.bias\", \"model.encoder.layers.0.3.net.2.weight\", \"model.encoder.layers.0.3.net.2.bias\", \"model.encoder.layers.1.0.weight\", \"model.encoder.layers.1.1.att.in_proj_weight\", \"model.encoder.layers.1.1.att.in_proj_bias\", \"model.encoder.layers.1.1.att.out_proj.weight\", \"model.encoder.layers.1.1.att.out_proj.bias\", \"model.encoder.layers.1.2.weight\", \"model.encoder.layers.1.3.net.0.weight\", \"model.encoder.layers.1.3.net.0.bias\", \"model.encoder.layers.1.3.net.2.weight\", \"model.encoder.layers.1.3.net.2.bias\", \"model.encoder.norm.weight\", \"model.encoder2.layers.0.0.weight\", \"model.encoder2.layers.0.1.att.in_proj_weight\", \"model.encoder2.layers.0.1.att.in_proj_bias\", \"model.encoder2.layers.0.1.att.out_proj.weight\", \"model.encoder2.layers.0.1.att.out_proj.bias\", \"model.encoder2.layers.0.2.weight\", \"model.encoder2.layers.0.3.net.0.weight\", \"model.encoder2.layers.0.3.net.0.bias\", \"model.encoder2.layers.0.3.net.2.weight\", \"model.encoder2.layers.0.3.net.2.bias\", \"model.encoder2.layers.1.0.weight\", \"model.encoder2.layers.1.1.att.in_proj_weight\", \"model.encoder2.layers.1.1.att.in_proj_bias\", \"model.encoder2.layers.1.1.att.out_proj.weight\", \"model.encoder2.layers.1.1.att.out_proj.bias\", \"model.encoder2.layers.1.2.weight\", \"model.encoder2.layers.1.3.net.0.weight\", \"model.encoder2.layers.1.3.net.0.bias\", \"model.encoder2.layers.1.3.net.2.weight\", \"model.encoder2.layers.1.3.net.2.bias\", \"model.encoder2.norm.weight\", \"model.decoder.weight\", \"model.decoder.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Backbone\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m Backbone(\u001b[38;5;241m3\u001b[39m, config)\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoints/ours/Model_ours_Dataset_financial_Multi_round_0.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Backbone:\n\tMissing key(s) in state_dict: \"projection.weight\", \"projection.bias\", \"position_embedding.pos_encoding.embedding.weight\", \"fund_embedding.weight\", \"predict_linear.weight\", \"predict_linear.bias\", \"encoder.layers.0.0.weight\", \"encoder.layers.0.1.att.in_proj_weight\", \"encoder.layers.0.1.att.in_proj_bias\", \"encoder.layers.0.1.att.out_proj.weight\", \"encoder.layers.0.1.att.out_proj.bias\", \"encoder.layers.0.2.weight\", \"encoder.layers.0.3.net.0.weight\", \"encoder.layers.0.3.net.0.bias\", \"encoder.layers.0.3.net.2.weight\", \"encoder.layers.0.3.net.2.bias\", \"encoder.layers.1.0.weight\", \"encoder.layers.1.1.att.in_proj_weight\", \"encoder.layers.1.1.att.in_proj_bias\", \"encoder.layers.1.1.att.out_proj.weight\", \"encoder.layers.1.1.att.out_proj.bias\", \"encoder.layers.1.2.weight\", \"encoder.layers.1.3.net.0.weight\", \"encoder.layers.1.3.net.0.bias\", \"encoder.layers.1.3.net.2.weight\", \"encoder.layers.1.3.net.2.bias\", \"encoder.norm.weight\", \"encoder2.layers.0.0.weight\", \"encoder2.layers.0.1.att.in_proj_weight\", \"encoder2.layers.0.1.att.in_proj_bias\", \"encoder2.layers.0.1.att.out_proj.weight\", \"encoder2.layers.0.1.att.out_proj.bias\", \"encoder2.layers.0.2.weight\", \"encoder2.layers.0.3.net.0.weight\", \"encoder2.layers.0.3.net.0.bias\", \"encoder2.layers.0.3.net.2.weight\", \"encoder2.layers.0.3.net.2.bias\", \"encoder2.layers.1.0.weight\", \"encoder2.layers.1.1.att.in_proj_weight\", \"encoder2.layers.1.1.att.in_proj_bias\", \"encoder2.layers.1.1.att.out_proj.weight\", \"encoder2.layers.1.1.att.out_proj.bias\", \"encoder2.layers.1.2.weight\", \"encoder2.layers.1.3.net.0.weight\", \"encoder2.layers.1.3.net.0.bias\", \"encoder2.layers.1.3.net.2.weight\", \"encoder2.layers.1.3.net.2.bias\", \"encoder2.norm.weight\", \"decoder.weight\", \"decoder.bias\". \n\tUnexpected key(s) in state_dict: \"model.projection.weight\", \"model.projection.bias\", \"model.position_embedding.pos_encoding.embedding.weight\", \"model.fund_embedding.weight\", \"model.predict_linear.weight\", \"model.predict_linear.bias\", \"model.encoder.layers.0.0.weight\", \"model.encoder.layers.0.1.att.in_proj_weight\", \"model.encoder.layers.0.1.att.in_proj_bias\", \"model.encoder.layers.0.1.att.out_proj.weight\", \"model.encoder.layers.0.1.att.out_proj.bias\", \"model.encoder.layers.0.2.weight\", \"model.encoder.layers.0.3.net.0.weight\", \"model.encoder.layers.0.3.net.0.bias\", \"model.encoder.layers.0.3.net.2.weight\", \"model.encoder.layers.0.3.net.2.bias\", \"model.encoder.layers.1.0.weight\", \"model.encoder.layers.1.1.att.in_proj_weight\", \"model.encoder.layers.1.1.att.in_proj_bias\", \"model.encoder.layers.1.1.att.out_proj.weight\", \"model.encoder.layers.1.1.att.out_proj.bias\", \"model.encoder.layers.1.2.weight\", \"model.encoder.layers.1.3.net.0.weight\", \"model.encoder.layers.1.3.net.0.bias\", \"model.encoder.layers.1.3.net.2.weight\", \"model.encoder.layers.1.3.net.2.bias\", \"model.encoder.norm.weight\", \"model.encoder2.layers.0.0.weight\", \"model.encoder2.layers.0.1.att.in_proj_weight\", \"model.encoder2.layers.0.1.att.in_proj_bias\", \"model.encoder2.layers.0.1.att.out_proj.weight\", \"model.encoder2.layers.0.1.att.out_proj.bias\", \"model.encoder2.layers.0.2.weight\", \"model.encoder2.layers.0.3.net.0.weight\", \"model.encoder2.layers.0.3.net.0.bias\", \"model.encoder2.layers.0.3.net.2.weight\", \"model.encoder2.layers.0.3.net.2.bias\", \"model.encoder2.layers.1.0.weight\", \"model.encoder2.layers.1.1.att.in_proj_weight\", \"model.encoder2.layers.1.1.att.in_proj_bias\", \"model.encoder2.layers.1.1.att.out_proj.weight\", \"model.encoder2.layers.1.1.att.out_proj.bias\", \"model.encoder2.layers.1.2.weight\", \"model.encoder2.layers.1.3.net.0.weight\", \"model.encoder2.layers.1.3.net.0.bias\", \"model.encoder2.layers.1.3.net.2.weight\", \"model.encoder2.layers.1.3.net.2.bias\", \"model.encoder2.norm.weight\", \"model.decoder.weight\", \"model.decoder.bias\". "
     ]
    }
   ],
   "source": [
    "from modules.backbone import Backbone\n",
    "import torch \n",
    "model = Backbone(3, config).load_state_dict(torch.load('checkpoints/ours/Model_ours_Dataset_financial_Multi_round_0.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "531b688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "df = np.array(all_history_input)\n",
    "df.shape\n",
    "#%%\n",
    "data = np.stack(df, axis=0)\n",
    "data = data.transpose(1, 0, 2)\n",
    "x = data[:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df7211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
