import torch
from data_provider.data_loader import DataModule
from exp.exp_model import Model
from data_provider.generate_financial import process_date_columns, query_fund_data
from data_provider.get_financial import get_group_idx
from run_train import get_experiment_name
from utils.exp_config import get_config
from sqlalchemy import create_engine, text
import numpy as np
import pandas as pd
import pickle
from sqlalchemy import create_engine
from sqlalchemy.exc import SQLAlchemyError
from datetime import datetime, timedelta
import os 
from utils.exp_logger import Logger
from utils.exp_metrics_plotter import MetricsPlotter

def drop_sql_temp(tabel_name):
    try:
        # è¯»å–æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
        with open('./datasets/sql_token.pkl', 'rb') as f:
            DB_URI = pickle.load(f)

        # åˆ›å»ºæ•°æ®åº“å¼•æ“
        engine = create_engine(DB_URI)

        # æ‰§è¡Œ DROP TABLE æ“ä½œ
        with engine.connect() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {tabel_name}"))
            print(f"âœ… è¡¨ {tabel_name} å·²æˆåŠŸåˆ é™¤ã€‚")

    except Exception as e:
        print(f"âŒ åˆ é™¤è¡¨æ—¶å‡ºé”™: {e}")

def get_start_date(end_date: str, window_size: int) -> str:
    """
    ç»™å®šç»“æŸæ—¥æœŸå’Œå†å²çª—å£é•¿åº¦ï¼Œè¿”å›çª—å£å¼€å§‹æ—¥æœŸï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰ã€‚

    å‚æ•°ï¼š
    - end_date (str): ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ 'YYYY-MM-DD'
    - window_size (int): å†å²çª—å£é•¿åº¦ï¼ˆå¤©æ•°ï¼‰

    è¿”å›ï¼š
    - start_date (str): å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ 'YYYY-MM-DD'
    """
    end_dt = datetime.strptime(end_date, "%Y-%m-%d")
    start_dt = end_dt - timedelta(days=window_size)
    return start_dt.strftime("%Y-%m-%d")

def check_bad_model(all_scnerios, log, config):
    all_acc = []
    try:
        for seq_len, pred_len in all_scnerios:
            config.seq_len = seq_len
            config.pred_len = pred_len
            filename, exper_detail = get_experiment_name(config)
            log.filename = filename
            with open(f'./results/metrics/{log.filename}.pkl', 'rb') as f:
                metrics = pickle.load(f)
                Acc = metrics['Acc_10']
                all_acc.append(Acc)
    except FileNotFoundError as e:
        print(f"âŒ {e}ï¼Œå¯èƒ½æ˜¯æ¨¡å‹æœªæ”¶æ•›ã€‚")
        return True
    
    return np.mean(all_acc) < 0.5

def get_history_data(get_group_idx, current_date, config):
    all_history_input = []
    start_date = get_start_date(current_date, window_size=2000)
    fund_dict = query_fund_data(get_group_idx, start_date, current_date)
    min_len = 1e9
    for key, value in fund_dict.items():
        min_len = min(len(value), min_len)

    for key, value in fund_dict.items():
        df = process_date_columns(value)
        df = df[-min_len:, :]
        all_history_input.append(df)
    data = all_history_input
    return data

def check_input(all_history_input, config):
    data = np.stack(all_history_input, axis=0)
    data = data.transpose(1, 0, 2)
    # åªå–ç¬¦åˆæ¨¡å‹çš„å†å²å¤©æ•°
    data = data[-config.seq_len:, :, :]
    return data

def get_pretrained_model(log, config):
    model = Model(config)
    runId = 0
    model_path = f'./checkpoints/{config.model}/{log.filename}_round_{runId}.pt'
    print(model_path)
    model.load_state_dict(torch.load(model_path, weights_only=True, map_location='cpu'))
    return model 


def apply_delta_with_hist_constraints(hist, pred):
    """
    hist: shape [seq_len, n]
    pred: shape [T_pred, n]
    """
    pred_clipped = pred.copy().T  # shape: [n, T_pred]
    hist = hist.copy().T          # shape: [n, seq_len]
    N, T_pred = pred_clipped.shape

    # Step 1: è®¡ç®—å†å²æœ€å¤§æ¶¨è·Œ
    hist_diff = hist[:, 1:] - hist[:, :-1]
    max_gain = np.max(hist_diff, axis=1)  # [n]
    max_drop = np.min(hist_diff, axis=1)  # [n]

    # print("åŸºé‡‘å†å²æœ€å¤§æ¶¨è·Œå¹…ï¼š")
    # for i in range(N):
        # print(f"åŸºé‡‘{i}: max_gain = {max_gain[i]:.4f}, max_drop = {max_drop[i]:.4f}")

    # Step 2: åº”ç”¨é€’æ¨çº¦æŸ
    for i in range(N):
        for t in range(1, T_pred):
            prev = pred_clipped[i, t - 1]
            curr = pred_clipped[i, t]
            delta = curr - prev

            if delta > max_gain[i]:
                low = prev
                high = prev + max_gain[i]
                if high < low:
                    low, high = high, low
                new_val = np.random.uniform(low, high)
                print(f"[åŸºé‡‘{i}] ç¬¬{t}æ­¥: æ¶¨å¹…è¶…é™ (Î”={delta:.4f} > {max_gain[i]:.4f})ï¼ŒåŸå€¼={curr:.4f} â†’ æ–°å€¼={new_val:.4f}")
                pred_clipped[i, t] = new_val

            elif delta < max_drop[i]:
                low = prev + max_drop[i]  # æ³¨æ„æ˜¯ prev + max_dropï¼ˆmax_drop æ˜¯è´Ÿæ•°ï¼‰
                high = prev
                if high < low:
                    low, high = high, low
                new_val = np.random.uniform(low, high)
                print(f"[åŸºé‡‘{i}] ç¬¬{t}æ­¥: è·Œå¹…è¶…é™ (Î”={delta:.4f} < {max_drop[i]:.4f})ï¼ŒåŸå€¼={curr:.4f} â†’ æ–°å€¼={new_val:.4f}")
                pred_clipped[i, t] = new_val

    return pred_clipped.T  # shape: [T_pred, n]

def predict_torch_model(model, history_input, x_scaler, y_scaler, config):
    # å› ä¸ºæˆ‘åŠ äº†æ—¶é—´æˆ³ç‰¹å¾
    history_input[:, :, 3:] = x_scaler.transform(history_input[:, :, 3:])
    x = history_input[:, :, -3:]
    x_fund = history_input[:, :, 0]
    x_mark = history_input[:, :, 1:4] 
    x_features = history_input[:, :, 4:-3]
    # unsqueeze ä»£è¡¨ batch size = 1
    x = torch.from_numpy(x.astype(np.float32)).unsqueeze(0)
    x_features = torch.from_numpy(x_features.astype(np.float32)).unsqueeze(0)
    pred_value = model(x, x_mark, x_fund, x_features).squeeze(0).detach().numpy()
    pred_value = y_scaler.inverse_transform(pred_value)
    pred_value = pred_value[:, :, -1]
    pred_value = np.abs(pred_value)
    return pred_value

def get_final_pred(all_scnerios, group_fund_code, current_date, log, config):

    history_input = get_history_data(group_fund_code, current_date, config)
    print(f"ğŸ“ˆ å†å²æ•°æ®å·²è·å–ã€‚åˆ—è¡¨é•¿åº¦: {len(history_input)}")
    all_pred = np.zeros((90, len(history_input)))

    prev_len = 0
    for seq_len, pred_len in all_scnerios:
        config.seq_len = seq_len
        config.pred_len = pred_len
        filename, exper_detail = get_experiment_name(config)
        datamodule = DataModule(config)
        x_scaler, y_scaler = datamodule.x_scaler, datamodule.y_scaler
        del datamodule
        log.filename = filename
        cleaned_input = check_input(history_input, config)
        print(f"ğŸ§¹ æ¸…æ´—åçš„è¾“å…¥æ•°æ®ç»´åº¦: {cleaned_input.shape}")  # åº”ä¸º [seq_len, group_num, feature_dim]

        model = get_pretrained_model(log, config)
        print("ğŸ¤– æ¨¡å‹åŠ è½½å®Œæˆã€‚")

        pred_value = predict_torch_model(model, cleaned_input, x_scaler, y_scaler, config)
        print(f"ğŸ“‰ é¢„æµ‹ç»“æœç»´åº¦: {pred_value.shape}")
        
        start_idx = prev_len
        end_idx = config.pred_len
        prev_len = config.pred_len
        all_pred[start_idx:end_idx, :] = pred_value[start_idx:end_idx, :]
        print(f"ğŸ“Š é¢„æµ‹ç»“æœå·²å­˜å…¥ all_predï¼Œä» {start_idx} åˆ° {end_idx}ã€‚")

    hist = np.stack(history_input, axis=0).transpose(1, 0, 2)[:, :, -1]
    pred_value = apply_delta_with_hist_constraints(hist, pred_value)
    return pred_value, cleaned_input

def get_sql_format_data(pred_value, cleaned_input):
    from datetime import datetime
    now_df = []
    cleaned_input = cleaned_input[0, :, :]
    for j in range(pred_value.shape[1]):
        idx = config.idx
        fund_code = cleaned_input[j][0]
        forcast_date = current_date
        pred = '{"pre": [' + ', '.join(f'{item:.6f}' for item in pred_value[:, j]) + ']}'
        model_version = 'v2025'
        create_date = update_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        now_df.append([idx, fund_code, forcast_date, pred, model_version, create_date, update_date])
        # break
    # now_df
    now_df = np.array(now_df)
    now_df = pd.DataFrame(now_df, columns=['id', 'fund_code', 'forecast_date', 'pre_data', 'model_version', 'create_time', 'update_time'])
    return now_df



def insert_pred_to_sql(df, table_name):
    try:
        # 1. First save to CSV in results/csv directory
        os.makedirs('./results/csv', exist_ok=True)  # Create directory if it doesn't exist
        csv_path = f'./results/csv/{table_name}_predictions.csv'
        df.to_csv(csv_path, index=False)
        print(f"âœ… æ•°æ®æˆåŠŸä¿å­˜åˆ°CSVæ–‡ä»¶: {csv_path}")
        
        # 2. Then proceed with SQL insertion as before
        # è¯»å–æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
        with open('./datasets/sql_token.pkl', 'rb') as f:
            DB_URI = pickle.load(f)
        # åˆ›å»ºæ•°æ®åº“å¼•æ“
        engine = create_engine(DB_URI)
        # æ’å…¥æ•°æ®
        df.to_sql(
            name=table_name,   # è¡¨å
            con=engine,                   # æ•°æ®åº“è¿æ¥
            if_exists='append',           # è¿½åŠ åˆ°å·²æœ‰è¡¨ä¸­
            index=False                   # ä¸æ’å…¥ç´¢å¼•åˆ—
        )
        print(f"âœ… æ•°æ®æˆåŠŸå†™å…¥æ•°æ®åº“{table_name}ã€‚")
        
    except FileNotFoundError:
        print("âŒ æ— æ³•æ‰¾åˆ° sql_token.pkl æ–‡ä»¶ã€‚è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")
    except SQLAlchemyError as e:
        print(f"âŒ æ•°æ®åº“æ’å…¥å¤±è´¥: {e}")
    except Exception as e:
        print(f"âŒ å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")


# [128, 16, 33, 3])
def start_server(current_date, table_name = 'temp_sql'):
    drop_sql_temp(table_name)

    print(f"\nğŸ“… å½“å‰é¢„æµ‹æ—¥æœŸ: {current_date}")
    print(f"â¡ï¸ è¾“å…¥åºåˆ—é•¿åº¦: {config.seq_len}, é¢„æµ‹é•¿åº¦: {config.pred_len}")
    
    with open('./datasets/func_code_to_label_160_balanced.pkl', 'rb') as f:
        data = np.array(pickle.load(f))
        df = data[:, 1].astype(np.float32)
    group_num = int(df.max() + 1)
    all_scnerios = [[36, 7], [36, 30], [36, 60], [36, 90]]

    for i in range(group_num):
        # 27
        try:
            log_filename, exper_detail = get_experiment_name(config)
            plotter = MetricsPlotter(log_filename, config)
            log = Logger(log_filename, exper_detail, plotter, config, show_params=False)

            config.idx = i
            group_fund_code = get_group_idx(i, config)
            print(f"ğŸ“Š è·å–åŸºé‡‘ç»„å…± {len(group_fund_code)} ä¸ªåŸºé‡‘åˆ—è¡¨ä¸­")

            if check_bad_model(all_scnerios, log, config):
                print(f"â—ï¸ æ¨¡å‹æ•ˆæœä¸ä½³ï¼Œè·³è¿‡åŸºé‡‘ç»„ {i} çš„é¢„æµ‹ã€‚")
                continue

            print(f"ğŸ” æ­£åœ¨å¤„ç†åŸºé‡‘ç»„ {i}ï¼Œéƒ¨åˆ†åŸºé‡‘ä»£ç : {group_fund_code[:10]}")
            pred_value, cleaned_input = get_final_pred(all_scnerios, group_fund_code, current_date, log, config)

            pred_value_sql = get_sql_format_data(pred_value, cleaned_input)
            print(f"ğŸ§¾ é¢„æµ‹ç»“æœå·²è½¬ä¸º DataFrameï¼Œå‡†å¤‡å†™å…¥æ•°æ®åº“ã€‚è¡¨æ ¼ shape: {pred_value_sql.shape}")
            print(pred_value_sql.head(5))  # æ‰“å°å‰ä¸¤è¡Œä»¥æ ¸éªŒå†…å®¹ç»“æ„
            
            insert_pred_to_sql(pred_value_sql, table_name)
        except Exception as e:
            raise e
            print(e)
            continue
    return pred_value_sql



if __name__ == '__main__':
    config = get_config('FinancialConfig')
    print("âœ… é…ç½®åŠ è½½å®Œæˆã€‚")
    current_date = datetime.now().strftime('%Y-%m-%d')
    pred_value = start_server(current_date)