import torch
from exp.exp_model import Model
from data_provider.generate_financial import get_all_fund_list, process_date_columns, query_fund_data
from data_provider.get_financial import get_group_idx
from run_train import get_experiment_name
from utils.exp_config import get_config
from sqlalchemy import create_engine, text
import numpy as np
import pandas as pd
import pickle
from sqlalchemy import create_engine
from sqlalchemy.exc import SQLAlchemyError
from datetime import datetime, timedelta

from utils.exp_logger import Logger
from utils.exp_metrics_plotter import MetricsPlotter

def drop_sql_temp(tabel_name):
    try:
        # è¯»å–æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
        with open('./datasets/sql_token.pkl', 'rb') as f:
            DB_URI = pickle.load(f)

        # åˆ›å»ºæ•°æ®åº“å¼•æ“
        engine = create_engine(DB_URI)

        # æ‰§è¡Œ DROP TABLE æ“ä½œ
        with engine.connect() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {tabel_name}"))
            print(f"âœ… è¡¨ {tabel_name} å·²æˆåŠŸåˆ é™¤ã€‚")

    except Exception as e:
        print(f"âŒ åˆ é™¤è¡¨æ—¶å‡ºé”™: {e}")

def get_start_date(end_date: str, window_size: int) -> str:
    """
    ç»™å®šç»“æŸæ—¥æœŸå’Œå†å²çª—å£é•¿åº¦ï¼Œè¿”å›çª—å£å¼€å§‹æ—¥æœŸï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰ã€‚

    å‚æ•°ï¼š
    - end_date (str): ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ 'YYYY-MM-DD'
    - window_size (int): å†å²çª—å£é•¿åº¦ï¼ˆå¤©æ•°ï¼‰

    è¿”å›ï¼š
    - start_date (str): å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ 'YYYY-MM-DD'
    """
    end_dt = datetime.strptime(end_date, "%Y-%m-%d")
    start_dt = end_dt - timedelta(days=window_size)
    return start_dt.strftime("%Y-%m-%d")

def get_history_data(now_code, current_date, config):
    start_date = get_start_date(current_date, window_size=64)
    data = query_fund_data([now_code], start_date, current_date)[now_code]
    data = np.array(data)
    return data

def check_input(single_history_input, config):
    data = single_history_input[-config.seq_len:, :]
    return data

def get_pretrained_model(log, config):
    model = Model(config)
    runId = 0
    model_path = f'./checkpoints/{config.model}/{log.filename}_round_{runId}.pt'
    print(model_path)
    # model.load_state_dict(torch.load(model_path, weights_only=True, map_location='cpu'))
    return model 


def constrain_nav_prediction(predictions, bar=0.05, scale=0.9):
    """
    æ£€æµ‹å•ä½å‡€å€¼é¢„æµ‹ä¸­æ˜¯å¦å­˜åœ¨è¶…è¿‡barçš„ç›¸é‚»æ¶¨è·Œå¹…ï¼Œ
    å¦‚æœæ˜¯ï¼Œåˆ™æ•´æ¡åŸºé‡‘çš„å‡€å€¼åºåˆ—æŒ‰ç›¸å¯¹é¦–æ—¥å€¼é‡æ–°ç¼©æ”¾ï¼ˆæ¸©å’Œè°ƒæ•´ï¼‰

    å‚æ•°ï¼š
    - predictions: np.ndarray [7, 64]ï¼Œè¡¨ç¤º64æ”¯åŸºé‡‘7å¤©çš„é¢„æµ‹å•ä½å‡€å€¼
    - bar: floatï¼Œå•ä½å‡€å€¼æ—¥æ¶¨è·Œå¹…ä¸Šé™ï¼ˆå¦‚0.05è¡¨ç¤º5%ï¼‰
    - scale: floatï¼Œæ£€æµ‹å¼‚å¸¸åï¼Œä½¿ç”¨çš„è¶‹åŠ¿ç¼©æ”¾ç³»æ•°ï¼ˆå¦‚0.9ï¼‰

    è¿”å›ï¼š
    - adjusted: np.ndarray [7, 64]ï¼Œå¤„ç†åçš„å•ä½å‡€å€¼é¢„æµ‹
    - mask: np.ndarray [64]ï¼Œè¡¨ç¤ºå“ªäº›åŸºé‡‘è¢«ç¼©æ”¾ï¼ˆTrueä¸ºç¼©æ”¾ï¼‰
    """
    adjusted = predictions.copy()
    mask = np.zeros(predictions.shape[1], dtype=bool)
    for fund_idx in range(predictions.shape[1]):
        nav_series = predictions[:, fund_idx]
        # è®¡ç®—ç›¸é‚»æ¶¨è·Œå¹…
        returns = nav_series[1:] / nav_series[:-1] - 1
        if np.any(np.abs(returns) > bar):
            # ä»¥é¦–æ—¥ä¸ºé”šç‚¹ï¼Œé‡æ„æ¸©å’Œæ›²çº¿
            base = nav_series[0]
            relative_change = (nav_series - base) / base
            softened = base * (1 + relative_change * scale)
            adjusted[:, fund_idx] = softened
            mask[fund_idx] = True
    return adjusted, mask

def predict_torch_model(model, history_input, config):
    # å› ä¸ºæˆ‘åŠ äº†æ—¶é—´æˆ³ç‰¹å¾
    x = history_input[:, -3:]
    # unsqueeze ä»£è¡¨ batch size = 1
    x = torch.from_numpy(x.astype(np.float32)).unsqueeze(0)
    pred_value = model(x, None, None).squeeze(0).detach().numpy()
    # å› ä¸ºæ¨¡å‹æ”¹æˆäº†å¤šå˜é‡é¢„æµ‹å¤šå˜é‡ï¼ŒæŒ‰ç…§é¢„æµ‹ç»“æœçš„æœ€åä¸€ä¸ªå˜é‡ä½œä¸ºé¢„æµ‹å€¼
    pred_value = pred_value[:, -1]
    pred_value = np.abs(pred_value)
    # pred_value, _ = constrain_nav_prediction(pred_value, bar=1.0, scale=0.1)
    return pred_value

def get_sql_format_data(pred_value, cleaned_input):
    from datetime import datetime
    now_df = []
    cleaned_input = cleaned_input[0, :]
    idx = config.idx
    fund_code = cleaned_input[0]
    forcast_date = current_date
    pred = '{"pre": [' + ', '.join(f'{item:.6f}' for item in pred_value[:]) + ']}'
    model_version = 'v2025'
    create_date = update_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    now_df.append([idx, fund_code, forcast_date, pred, model_version, create_date, update_date])
    now_df = np.array(now_df)
    now_df = pd.DataFrame(now_df, columns=['id', 'fund_code', 'forecast_date', 'pre_data', 'model_version',
       'create_time', 'update_time'])
    return now_df

def insert_pred_to_sql(df, table_name):
    try:
        # è¯»å–æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
        with open('./datasets/sql_token.pkl', 'rb') as f:
            DB_URI = pickle.load(f)

        # åˆ›å»ºæ•°æ®åº“å¼•æ“
        engine = create_engine(DB_URI)

        # æ’å…¥æ•°æ®
        df.to_sql(
            name=table_name,   # è¡¨å
            con=engine,                   # æ•°æ®åº“è¿æ¥
            if_exists='append',           # è¿½åŠ åˆ°å·²æœ‰è¡¨ä¸­
            index=False                   # ä¸æ’å…¥ç´¢å¼•åˆ—
        )
        print(f"âœ… æ•°æ®æˆåŠŸå†™å…¥æ•°æ®åº“{table_name}ã€‚")

    except FileNotFoundError:
        print("âŒ æ— æ³•æ‰¾åˆ° sql_token.pkl æ–‡ä»¶ã€‚è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")

    except SQLAlchemyError as e:
        print(f"âŒ æ•°æ®åº“æ’å…¥å¤±è´¥: {e}")

    except Exception as e:
        print(f"âŒ å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

# [128, 16, 33, 3])
def start_server(current_date, table_name = 'temp_sql'):
    drop_sql_temp(table_name)
    print("âœ… é…ç½®åŠ è½½å®Œæˆã€‚")

    print(f"\nğŸ“… å½“å‰é¢„æµ‹æ—¥æœŸ: {current_date}")
    print(f"â¡ï¸ è¾“å…¥åºåˆ—é•¿åº¦: {config.seq_len}, é¢„æµ‹é•¿åº¦: {config.pred_len}")
    
    all_code_list = get_all_fund_list()
    for i in range(len(all_code_list)):
        # 27
        try:
            log_filename, exper_detail = get_experiment_name(config)
            plotter = MetricsPlotter(log_filename, config)
            log = Logger(log_filename, exper_detail, plotter, config, show_params=False)
            config.idx = i
            print(f"ğŸ“Š è·å–åŸºé‡‘ç»„ç¬¬ {i} ä¸ªåŸºé‡‘ï¼ŒåŸºé‡‘å·ä¸º {all_code_list[i]}")

            history_input = get_history_data(all_code_list[i], current_date, config)
            print(f"ğŸ“ˆ å†å²æ•°æ®å·²è·å–ã€‚åˆ—è¡¨é•¿åº¦: {len(history_input)}")

            cleaned_input = check_input(history_input, config)
            print(f"ğŸ§¹ æ¸…æ´—åçš„è¾“å…¥æ•°æ®ç»´åº¦: {cleaned_input.shape}")  # åº”ä¸º [seq_len, group_num, feature_dim]

            model = get_pretrained_model(log, config)
            print("ğŸ¤– æ¨¡å‹åŠ è½½å®Œæˆã€‚")

            pred_value = predict_torch_model(model, cleaned_input, config)
            print(f"ğŸ“‰ é¢„æµ‹ç»“æœç»´åº¦: {pred_value.shape}")

            pred_value_sql = get_sql_format_data(pred_value, cleaned_input)
            print(f"ğŸ§¾ é¢„æµ‹ç»“æœå·²è½¬ä¸º DataFrameï¼Œå‡†å¤‡å†™å…¥æ•°æ®åº“ã€‚è¡¨æ ¼ shape: {pred_value_sql.shape}")
            print(pred_value_sql.head(2))  # æ‰“å°å‰ä¸¤è¡Œä»¥æ ¸éªŒå†…å®¹ç»“æ„

            insert_pred_to_sql(pred_value_sql, table_name)
        except Exception as e:
            raise e
            print(e)
            continue

    return pred_value_sql

if __name__ == '__main__':
    config = get_config('FinancialConfig')
    # current_date = '2025-4-15'
    current_date = '2025-7-02'
    pred_value = start_server(current_date)